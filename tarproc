#!/usr/bin/env python3
#
# Copyright (c) 2017-2019 NVIDIA CORPORATION. All rights reserved.
# This file is part of webloader (see TBD).
# See the LICENSE file for licensing terms (BSD-style).
#

import argparse
import atexit
import glob
import os
import shutil
import subprocess
import sys
from multiprocessing import Pool

from tarproclib import paths, reader, writer

epilog = """
Run a command line tool over all samples.

Each sample is extracted into its own directory with
a common basename (default=sample) and the extensions from the sample.

Example:

    tarproc -I png -c 'convert sample.jpg sample.png' inputs.tar -o outputs.tar
"""

parser = argparse.ArgumentParser(
    formatter_class=argparse.RawDescriptionHelpFormatter,
    description="Run commands over all samples.",
    epilog=epilog,
)

parser.add_argument(
    "-v", "--verbose", action="store_true", help="output extra information"
)
parser.add_argument("--debug", action="store_true", help="output debugging information")
parser.add_argument(
    "-O", "--command-output", default="/dev/stderr", help="where to put command output"
)
parser.add_argument("-q", "--silent", action="store_true", help="extra quiet")
parser.add_argument(
    "-c",
    "--command",
    default=None,
    help="command to run for each sample (working dir = sample)",
)
parser.add_argument(
    "-s",
    "--script",
    default=None,
    help="script to run for each sample (working dir = sample)",
)
parser.add_argument(
    "-w", "--working_dir", default="__{pid}__", help="temporary working dir"
)
parser.add_argument(
    "-b",
    "--base",
    default="sample",
    help='base to substitute for __key__ (default="sample")',
)
parser.add_argument(
    "-S",
    "--subdirs",
    action="store_true",
    help="collect subdirectories into new samples",
)
parser.add_argument(
    "-p", "--parallel", default=0, type=int, help="execute scripts in parallel"
)
parser.add_argument(
    "-e",
    "--error-handling",
    default="skip",
    help="how to handle errors in scripts (ignore, skip, abort)",
)
parser.add_argument(
    "--interpreter", default="bash", help="interpreter used for script argument"
)
parser.add_argument(
    "--count",
    type=int,
    default=1000000000,
    help="stop after processing this many samples",
)
parser.add_argument(
    "--add-log",
    default="",
    help="add process output to the data record with this key/extension",
)
parser.add_argument("-o", "--output", default=None)
parser.add_argument(
    "--mode",
    default="tar",
    help="tar, lines (newline separated text), pages (^L separated text)",
)
parser.add_argument("input", default="-", nargs="?")
args = parser.parse_args()


def dprint(*vars, **kw):
    if args.debug:
        print("DEBUG", *vars, file=sys.stderr, **kw)


command = None


if args.command_output != "/dev/null":
    output_stream = open(args.command_output, "wb")


def close_output():
    global output_stream
    if output_stream is not None:
        output_stream.close()


atexit.register(close_output)

if args.script:
    assert not args.command
    command = [args.interpreter, os.path.abspath(args.script)]
elif args.command:
    assert not args.script
    command = [args.interpreter, "-c", args.command]
else:
    sys.exit("most provide either --command or --script")


def proc_sample(sample, index=0):
    assert isinstance(sample, dict)

    dprint("proc_sample")

    # process in a subdirectory
    dirname = os.path.join(args.working_dir, "_%08d" % index)
    os.mkdir(dirname)

    samples = []

    with paths.ChDir(dirname):

        # write the sample out as files
        for k, v in sample.items():
            fname = args.base + "." + k if k[0] != "_" else k
            dprint("writing", fname)
            paths.write_binary(fname, v)

        # execute the command and handle errors
        try:
            log_output = subprocess.check_output(command, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as exn:
            if args.error_handling == "ignore":
                if not args.silent:
                    dprint(
                        "exit status (ignore):",
                        exn.returncode,
                        sample.get("__key__", "?"),
                    )
                log_output = exn.output
                pass
            elif args.error_handling == "skip":
                if not args.silent:
                    dprint(
                        "exit status (skip)", exn.returncode, sample.get("__key__", "?")
                    )
                return []
            else:
                if not args.silent:
                    dprint(
                        "exit status (abort)",
                        exn.returncode,
                        sample.get("__key__", "?"),
                    )
                    dprint(exn.output)
                raise exn

        if output_stream is not None:
            output_stream.write(log_output)
            output_stream.write(b"\n")
            output_stream.flush()

        # gather up the result files
        files = sorted([fname for fname in glob.glob("*.*") if os.path.isfile(fname)])

        dprint("got", files)

        # processing may have produced multiple outputs; gather them up separately

        if args.subdirs:
            directories = [d for d in glob.glob("*") if os.path.isdir(d)]
        else:
            directories = ["."]

        for directory in directories:
            sample = {}
            with paths.ChDir(directory):
                with open("__key__") as stream:
                    sample_key = stream.readline().strip()
                sample["__key__"] = sample_key
                files = sorted(glob.glob(args.base + "*"))
                for fname in files:
                    key = paths.fullext(fname)
                    value = paths.read_binary(fname)
                    sample[key] = value
                if args.add_log != "":
                    sample[args.add_log] = log_output
            samples.append(sample)
    shutil.rmtree(dirname)
    for sample in samples:
        assert isinstance(sample, dict), sample
    return samples


def proc_sample1(arg):
    i, sample = arg
    result = proc_sample(sample, index=i)
    assert isinstance(result, list)
    for sample in result:
        assert isinstance(sample, dict), sample
    return result


args.working_dir = args.working_dir.format(pid=str(os.getpid()))

assert not os.path.exists(args.working_dir)
os.mkdir(args.working_dir)

atexit.register(lambda: shutil.rmtree(args.working_dir))

sink = None

if args.output is not None:
    sink = writer.TarWriter(args.output)
    atexit.register(lambda: sink.close())


def line_iterator(fname):
    # TODO we don't use gopen here because it doesn't handle text files
    if fname == "-":
        fname = "/dev/stdin"
    with open(fname) as stream:
        count = 0
        for line in stream.readlines():
            yield dict(__key__=f"{count}", txt=line)
            count += 1


def key_iterator(fname):
    if fname == "-":
        fname = "/dev/stdin"
    with open(fname) as stream:
        for line in stream.readlines():
            yield dict(__key__=line.strip())


def page_iterator(fname):
    if fname == "-":
        fname = "/dev/stdin"
    with open(fname) as stream:
        buf = ""
        count = 0
        for line in stream.readlines():
            if line == "\f\n":
                yield dict(__key__=f"{count}", txt=buf)
                buf = ""
            buf += line
        if buf != "":
            yield dict(__key__=f"{count}", txt=buf)


def make_source(fname):
    if args.mode == "tar":
        return enumerate(reader.TarIterator(fname))
    elif args.mode == "keys":
        return enumerate(key_iterator(fname))
    elif args.mode == "lines":
        return enumerate(line_iterator(fname))
    elif args.mode == "pages":
        return enumerate(page_iterator(fname))
    else:
        sys.exit(f"{args.mode}: unknown mode (should be: tar, lines, or pages)")


def handle_result(new_samples):
    assert isinstance(new_samples, list), new_samples
    global sink
    if args.verbose:
        for s in new_samples:
            assert isinstance(s, dict), s
            assert "__key__" in s, s.keys()
            keyinfo = [k for k in s.keys() if k[0] != "_"]
            dprint(s.get("__key__"), " ".join(keyinfo))
    if sink is not None:
        for s in new_samples:
            assert isinstance(s, dict), s
            assert "__key__" in s, s.keys()
            sink.write(s)


if args.parallel == 0:
    count = 0
    for i, sample in make_source(args.input):
        if count >= args.count:
            break
        assert isinstance(sample, dict)
        new_samples = proc_sample1((i, sample))
        handle_result(new_samples)
        count += 1
elif args.parallel > 0:
    count = 0
    with Pool(processes=args.parallel) as pool:
        for new_samples in pool.imap_unordered(proc_sample1, make_source(args.input)):
            if count >= args.count:
                break
            handle_result(new_samples)
            count += 1
